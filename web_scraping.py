from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import json
import csv

# Seleniumã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®šï¼ˆç”»é¢è¡¨ç¤ºãªã—ï¼‰
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
# User-Agentã‚’è¿½åŠ ï¼ˆé‡è¦ï¼‰
options.add_argument(
    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

# ãƒ‰ãƒ©ã‚¤ãƒãƒ¼èµ·å‹•
driver = webdriver.Chrome(service=Service(
    ChromeDriverManager().install()), options=options)

# --- CSVã‹ã‚‰èª­ã¿è¾¼ã¿ ---
plant_pages = []
try:
  with open("all_plants_urls.csv", newline='', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    for row in reader:
      plant_pages.append(row)
  print(f" CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ {len(plant_pages)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
except FileNotFoundError:
  print("âŒ all_plants_urls.csv ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
  driver.quit()
  exit()

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã‚’ä¿å­˜ã™ã‚‹è¾æ›¸
scraped_data = {}

# å„æ¤ç‰©ã®ãƒšãƒ¼ã‚¸ã‚’é †ç•ªã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
for i, row in enumerate(plant_pages):
  # CSVã®ã‚«ãƒ©ãƒ åã‚’ç¢ºèªã—ã¦é©åˆ‡ã«ã‚¢ã‚¯ã‚»ã‚¹
  if 'plant_name' in row and 'url' in row:
    plant_name = row['plant_name']
    url = row['url']
  elif 'name' in row and 'url' in row:
    plant_name = row['name']
    url = row['url']
  else:
    # CSVã®ã‚«ãƒ©ãƒ åãŒä¸æ˜ãªå ´åˆã€æœ€åˆã®2ã¤ã®ã‚«ãƒ©ãƒ ã‚’ä½¿ç”¨
    keys = list(row.keys())
    if len(keys) >= 2:
      plant_name = row[keys[0]]
      url = row[keys[1]]
    else:
      print(f"âŒ è¡Œ {i+1}: CSVã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“")
      continue

  print(f"\n{'='*50}")
  print(f"ğŸŒ± {plant_name} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹ ({i+1}/{len(plant_pages)})")
  print(f"URL: {url}")
  print(f"{'='*50}")

  try:
    # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
    driver.get(url)

    # JavaScriptèª­ã¿è¾¼ã¿ã®å¾…æ©Ÿ
    time.sleep(5)

    # ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    # ãƒ‡ãƒãƒƒã‚°: åŸºæœ¬æƒ…å ±ã‚’ç¢ºèª
    print(f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {soup.title.string if soup.title else 'ãªã—'}")
    print(f"HTMLå…¨ä½“ã®ã‚µã‚¤ã‚º: {len(html)} æ–‡å­—")

    # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ç¢ºèª
    body = soup.find_all("div", class_="article__body")
    print(f"article__body ã‚¯ãƒ©ã‚¹ã®è¦ç´ æ•°: {len(body)}")

    if len(body) == 0:
      print("ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã—ã¦ã„ã¾ã™...")
      # ä»–ã®å¯èƒ½æ€§ã®ã‚ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
      alternative_selectors = [
          ("div", "single__section__body"),
          ("div", "entry-content"),
          ("div", "post-content"),
          ("div", "content"),
          ("article", None),
          ("main", None),
      ]

      for tag, class_name in alternative_selectors:
        if class_name:
          elements = soup.find_all(tag, class_=class_name)
        else:
          elements = soup.find_all(tag)

        if elements:
          for elem in elements:
            paragraphs = elem.find_all("p")
            if paragraphs:
              print(
                  f"âœ… {tag}.{class_name if class_name else 'ãªã—'}: {len(paragraphs)} å€‹ã®æ®µè½ã‚’ç™ºè¦‹")
              body = elements  # è¦‹ã¤ã‹ã£ãŸã‚‰ä½¿ç”¨
              break
          if body:
            break

    # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    plant_content = []
    text_found = False

    for section in body:
      for elem in section.find_all(["h2", "h3", "p"]):
        text = elem.get_text().strip()
        if text:
          plant_content.append(text)
          text_found = True

    if text_found:
      print(f"âœ… {plant_name}: {len(plant_content)} å€‹ã®ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’å–å¾—")
      scraped_data[plant_name] = plant_content

    else:
      print(f"âŒ {plant_name}: ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
      scraped_data[plant_name] = []

      # å…¨ã¦ã®pã‚¿ã‚°ã‚’ç¢ºèª
      all_p_tags = soup.find_all("p")
      if all_p_tags:
        print(f"ãƒšãƒ¼ã‚¸å†…ã®å…¨pã‚¿ã‚°æ•°: {len(all_p_tags)}")
        for j, p in enumerate(all_p_tags[:3]):
          text = p.get_text().strip()
          if text:
            print(f"p{j+1}: {text[:100]}...")

  except Exception as e:
    print(f"âŒ {plant_name} ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
    scraped_data[plant_name] = []

  # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ã‚’è¨­ã‘ã‚‹
  time.sleep(2)

# ãƒ‰ãƒ©ã‚¤ãƒãƒ¼çµ‚äº†
driver.quit()

# çµæœã‚’ã¾ã¨ã‚ã¦è¡¨ç¤º
print(f"\n{'='*60}")
print("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã¾ã¨ã‚")
print(f"{'='*60}")

for plant_name, content in scraped_data.items():
  print(f"\nğŸŒ± {plant_name}:")
  print(f"   å–å¾—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(content)}")
  if content:
    print(f"   æœ€åˆã®ãƒ†ã‚­ã‚¹ãƒˆ: {content[0][:100]}...")
  else:
    print("   âŒ ãƒ†ã‚­ã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")

# JSONã¨ã—ã¦ä¿å­˜
with open("all_plants_data.json", "w", encoding="utf-8") as f:
  json.dump(scraped_data, f, ensure_ascii=False, indent=2)

print("\nâœ… å®Œäº†ï¼šall_plant_data.json ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ")

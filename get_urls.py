from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import csv

# --- Seleniumã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š ---
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument(
    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

driver = webdriver.Chrome(service=Service(
    ChromeDriverManager().install()), options=options)

# å…¨ãƒšãƒ¼ã‚¸ã®æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
all_plant_data = []

# --- 1ã‹ã‚‰14ãƒšãƒ¼ã‚¸ã¾ã§ãƒ«ãƒ¼ãƒ—å‡¦ç† ---
for page_num in range(1, 15):  # 1ã‹ã‚‰14ãƒšãƒ¼ã‚¸
    print(f"\n{'='*60}")
    print(f"ğŸ”„ ãƒšãƒ¼ã‚¸ {page_num}/14 ã‚’å‡¦ç†ä¸­")
    print(f"{'='*60}")
    
    # --- LOVEGREEN é‡èœä¸€è¦§ãƒšãƒ¼ã‚¸ ---
    if page_num == 1:
        url = "https://lovegreen.net/library/type/vegetables/page/1/"
    else:
        url = f"https://lovegreen.net/library/type/vegetables/page/{page_num}/"

    print(f"URL: {url}")
    
    try:
        driver.get(url)

        # JavaScriptã®èª­ã¿è¾¼ã¿ã‚’å¾…ã¤
        time.sleep(8)  # å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´

        # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—
        html = driver.page_source
        soup = BeautifulSoup(html, "html.parser")

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        print("=== ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===")
        print(f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {soup.title.string if soup.title else 'ãªã—'}")
        print(f"HTMLå…¨ä½“ã®ã‚µã‚¤ã‚º: {len(html)} æ–‡å­—")

        # å…ƒã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ç¢ºèª
        cards = soup.find_all("a", class_="library-list__item")
        print(f"library-list__item ã‚¯ãƒ©ã‚¹: {len(cards)} å€‹")

        if len(cards) == 0:
            print("\n=== ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã—ã¦ã„ã¾ã™ ===")
            
            # ä»–ã®å¯èƒ½æ€§ã®ã‚ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
            alternative_selectors = [
                ("a", "card"),
                ("a", "item"),
                ("div", "card"),
                ("article", None),
                ("div", "item"),
                ("a", "library-item"),
                ("div", "library-list"),
                ("a", "plant-card"),
            ]
            
            for tag, class_name in alternative_selectors:
                if class_name:
                    elements = soup.find_all(tag, class_=class_name)
                    print(f"{tag}.{class_name}: {len(elements)} å€‹")
                else:
                    elements = soup.find_all(tag)
                    print(f"{tag}: {len(elements)} å€‹")
                
                if elements and len(elements) > 0:
                    # æœ€åˆã®è¦ç´ ã‚’è©³ã—ãç¢ºèª
                    first_elem = elements[0]
                    print(f"  æœ€åˆã®è¦ç´ : {first_elem.name}")
                    print(f"  ã‚¯ãƒ©ã‚¹: {first_elem.get('class', [])}")
                    print(f"  href: {first_elem.get('href', 'ãªã—')}")
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’ç¢ºèª
                    text = first_elem.get_text(strip=True)
                    if text:
                        print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text[:50]}...")
                    
                    # ã“ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¦ã¿ã‚‹
                    if first_elem.get('href'):
                        cards = elements
                        break

        # å…¨ã¦ã®aã‚¿ã‚°ã‚’ç¢ºèª
        print(f"\n=== å…¨ã¦ã®aã‚¿ã‚°ã‚’ç¢ºèª ===")
        all_a_tags = soup.find_all("a")
        print(f"ãƒšãƒ¼ã‚¸å†…ã®å…¨aã‚¿ã‚°æ•°: {len(all_a_tags)}")

        # hrefã‚’æŒã¤aã‚¿ã‚°ã‚’ç¢ºèª
        a_with_href = [a for a in all_a_tags if a.get('href')]
        print(f"hrefã‚’æŒã¤aã‚¿ã‚°æ•°: {len(a_with_href)}")

        # vegetablesã«é–¢é€£ã™ã‚‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        vegetable_links = []
        for a in a_with_href:
            href = a.get('href')
            if href and 'vegetables' in href:
                vegetable_links.append(a)

        print(f"vegetablesã‚’å«ã‚€ãƒªãƒ³ã‚¯æ•°: {len(vegetable_links)}")

        # æœ€åˆã®10å€‹ã®vegetableãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
        print("\n=== vegetablesãƒªãƒ³ã‚¯ã®æœ€åˆã®10å€‹ ===")
        for i, link in enumerate(vegetable_links[:10]):
            href = link.get('href')
            text = link.get_text(strip=True)
            print(f"{i+1}. {text[:30]}... - {href}")

        # # HTMLãƒ€ãƒ³ãƒ—ã‚’ä¿å­˜ï¼ˆæœ€åˆã®3ãƒšãƒ¼ã‚¸ã®ã¿ï¼‰
        # if page_num <= 3:
        #     with open(f"vegetables_page_{page_num}_dump.html", "w", encoding="utf-8") as f:
        #         f.write(soup.prettify())
        #     print(f"\nğŸ’¾ vegetables_page_{page_num}_dump.html ã‚’ä¿å­˜ã—ã¾ã—ãŸ")

        # ã“ã®ãƒšãƒ¼ã‚¸ã®æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        page_plant_data = []

        # vegetablesãƒªãƒ³ã‚¯ã‹ã‚‰æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        for link in vegetable_links:
            href = link.get("href")
            text = link.get_text(strip=True)
            
            # å€‹åˆ¥ã®æ¤ç‰©ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            if href and text and len(text) > 0:
                # å®Œå…¨ãªURLã«ã™ã‚‹
                if href.startswith('/'):
                    full_url = "https://lovegreen.net" + href
                elif not href.startswith('http'):
                    full_url = "https://lovegreen.net/library/vegetables/" + href
                else:
                    full_url = href
                
                # ä¸€è¦§ãƒšãƒ¼ã‚¸è‡ªä½“ã¯é™¤å¤–
                if (not full_url.endswith('/vegetables/') and 
                    '/page/' not in full_url and
                    '/category/' not in full_url):
                    page_plant_data.append({"name": text, "url": full_url})
                    print(f"âœ“ {text} - {full_url}")

        # å…ƒã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚‚è©¦ã™
        if len(cards) > 0:
            print(f"\n=== å…ƒã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ã®æŠ½å‡º ===")
            for card in cards:
                link = card.get("href")
                name_tag = card.find("span", class_="library-list__item__title")
                name = name_tag.get_text(strip=True) if name_tag else None

                if name and link:
                    full_url = link if link.startswith("http") else "https://lovegreen.net" + link
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if not any(plant['url'] == full_url for plant in page_plant_data):
                        page_plant_data.append({"name": name, "url": full_url})
                        print(f"âœ“ {name} - {full_url}")

        # ã“ã®ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ä½“ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆé‡è¤‡é™¤å»ï¼‰
        new_count = 0
        existing_urls = {plant['url'] for plant in all_plant_data}
        
        for plant in page_plant_data:
            if plant['url'] not in existing_urls:
                all_plant_data.append(plant)
                new_count += 1
        
        print(f"\nğŸ“Š ãƒšãƒ¼ã‚¸ {page_num}: {new_count}ä»¶ã®æ–°ã—ã„é‡èœã‚’è¿½åŠ  (ç´¯è¨ˆ: {len(all_plant_data)}ä»¶)")
        
    except Exception as e:
        print(f"âŒ ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        continue

# ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ‚äº†
driver.quit()

print(f"\n{'='*60}")
print("ğŸ‰ å…¨ãƒšãƒ¼ã‚¸ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
print(f"{'='*60}")

# é‡è¤‡ã‚’é™¤å»ï¼ˆæœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼‰
unique_plants = []
seen_urls = set()
for plant in all_plant_data:
    if plant['url'] not in seen_urls:
        unique_plants.append(plant)
        seen_urls.add(plant['url'])

plant_data = unique_plants

# --- CSVã«ä¿å­˜ ---
if plant_data:
    with open("plant_urls.csv", "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["name", "url"])
        writer.writeheader()
        writer.writerows(plant_data)

    print(f"\nâœ… å®Œäº†ï¼š{len(plant_data)} ä»¶ã®é‡èœãƒ‡ãƒ¼ã‚¿ã‚’ plant_urls.csv ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    # æœ€åˆã®5ä»¶ã‚’è¡¨ç¤º
    print("\nğŸ“‹ æœ€åˆã®5ä»¶:")
    for i, plant in enumerate(plant_data[:5]):
        print(f"  {i+1}. {plant['name']} - {plant['url']}")
        
    # æœ€å¾Œã®5ä»¶ã‚’è¡¨ç¤º
    print("\nğŸ“‹ æœ€å¾Œã®5ä»¶:")
    for i, plant in enumerate(plant_data[-5:]):
        print(f"  {len(plant_data)-4+i}. {plant['name']} - {plant['url']}")
else:
    print("\nâŒ æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    print("ğŸ” vegetables_page_*_dump.html ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import csv

# --- Seleniumã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³è¨­å®š ---
options = Options()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument(
    '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

driver = webdriver.Chrome(service=Service(
    ChromeDriverManager().install()), options=options)

# å…¨ãƒšãƒ¼ã‚¸ã®æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
all_plant_data = []

# --- æ¤ç‰©ã‚«ãƒ†ã‚´ãƒªã®è¨­å®š ---
plant_categories = [
    {
        "name": "é‡èœ",
        "type": "vegetables",
        "url_base": "https://lovegreen.net/library/type/vegetables/",
        "max_pages": 14
    },
    {
        "name": "æœæ¨¹",
        "type": "fruit-tree",
        "url_base": "https://lovegreen.net/library/type/fruit-tree/",
        "max_pages": 5
    },
    {
        "name": "èŠ±",
        "type": "flower",
        "url_base": "https://lovegreen.net/library/type/flower/",
        "max_pages": 62
    }
]

# --- å„ã‚«ãƒ†ã‚´ãƒªã®å‡¦ç† ---
for category in plant_categories:
  print(f"\n{'='*80}")
  print(f"ğŸŒ± {category['name']}ã®å‡¦ç†ã‚’é–‹å§‹")
  print(f"{'='*80}")

  # --- å„ã‚«ãƒ†ã‚´ãƒªã®ãƒšãƒ¼ã‚¸ãƒ«ãƒ¼ãƒ—å‡¦ç† ---
  for page_num in range(1, category['max_pages'] + 1):
    print(f"\n{'='*60}")
    print(f"ğŸ”„ {category['name']} - ãƒšãƒ¼ã‚¸ {page_num}/{category['max_pages']} ã‚’å‡¦ç†ä¸­")

    # --- LOVEGREEN ä¸€è¦§ãƒšãƒ¼ã‚¸ ---
    if page_num == 1:
      if category['type'] == 'vegetables':
        url = f"{category['url_base']}page/1"
      elif category['type'] == 'fruit-tree':
        url = f"{category['url_base']}page/1"
      else:  # flower
        url = category['url_base']
    else:
      url = f"{category['url_base']}page/{page_num}/"

    print(f"URL: {url}")

    try:
      driver.get(url)

      # JavaScriptã®èª­ã¿è¾¼ã¿ã‚’å¾…ã¤
      time.sleep(8)  # å¾…æ©Ÿæ™‚é–“ã‚’èª¿æ•´

      # ãƒšãƒ¼ã‚¸å†…å®¹ã‚’å–å¾—
      html = driver.page_source
      soup = BeautifulSoup(html, "html.parser")

      # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
      print("=== ãƒ‡ãƒãƒƒã‚°æƒ…å ± ===")
      print(f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {soup.title.string if soup.title else 'ãªã—'}")
      print(f"HTMLå…¨ä½“ã®ã‚µã‚¤ã‚º: {len(html)} æ–‡å­—")

      # å…ƒã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ç¢ºèª
      cards = soup.find_all("a", class_="library-list__item")
      print(f"library-list__item ã‚¯ãƒ©ã‚¹: {len(cards)} å€‹")

      if len(cards) == 0:
        print("\n=== ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã—ã¦ã„ã¾ã™ ===")

        # ä»–ã®å¯èƒ½æ€§ã®ã‚ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
        alternative_selectors = [
            ("a", "card"),
            ("a", "item"),
            ("div", "card"),
            ("article", None),
            ("div", "item"),
            ("a", "library-item"),
            ("div", "library-list"),
            ("a", "plant-card"),
        ]

        for tag, class_name in alternative_selectors:
          if class_name:
            elements = soup.find_all(tag, class_=class_name)
            print(f"{tag}.{class_name}: {len(elements)} å€‹")
          else:
            elements = soup.find_all(tag)
            print(f"{tag}: {len(elements)} å€‹")

          if elements and len(elements) > 0:
            # æœ€åˆã®è¦ç´ ã‚’è©³ã—ãç¢ºèª
            first_elem = elements[0]
            print(f"  æœ€åˆã®è¦ç´ : {first_elem.name}")
            print(f"  ã‚¯ãƒ©ã‚¹: {first_elem.get('class', [])}")
            print(f"  href: {first_elem.get('href', 'ãªã—')}")

            # ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹ã‚’ç¢ºèª
            text = first_elem.get_text(strip=True)
            if text:
              print(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {text[:50]}...")

            # ã“ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¦ã¿ã‚‹
            if first_elem.get('href'):
              cards = elements
              break

      # å…¨ã¦ã®aã‚¿ã‚°ã‚’ç¢ºèª
      print(f"\n=== å…¨ã¦ã®aã‚¿ã‚°ã‚’ç¢ºèª ===")
      all_a_tags = soup.find_all("a")
      print(f"ãƒšãƒ¼ã‚¸å†…ã®å…¨aã‚¿ã‚°æ•°: {len(all_a_tags)}")

      # hrefã‚’æŒã¤aã‚¿ã‚°ã‚’ç¢ºèª
      a_with_href = [a for a in all_a_tags if a.get('href')]
      print(f"hrefã‚’æŒã¤aã‚¿ã‚°æ•°: {len(a_with_href)}")

      # ã‚«ãƒ†ã‚´ãƒªã«é–¢é€£ã™ã‚‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
      category_links = []
      for a in a_with_href:
        href = a.get('href')
        if href and category['type'] in href:
          category_links.append(a)

      print(f"{category['type']}ã‚’å«ã‚€ãƒªãƒ³ã‚¯æ•°: {len(category_links)}")

      # æœ€åˆã®5å€‹ã®ã‚«ãƒ†ã‚´ãƒªãƒªãƒ³ã‚¯ã‚’è¡¨ç¤º
      print(f"\n=== {category['type']}ãƒªãƒ³ã‚¯ã®æœ€åˆã®5å€‹ ===")
      for i, link in enumerate(category_links[:5]):
        href = link.get('href')
        text = link.get_text(strip=True)
        print(f"{i+1}. {text[:30]}... - {href}")

      # ã“ã®ãƒšãƒ¼ã‚¸ã®æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
      page_plant_data = []

      # ã‚«ãƒ†ã‚´ãƒªãƒªãƒ³ã‚¯ã‹ã‚‰æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
      for link in category_links:
        href = link.get("href")
        text = link.get_text(strip=True)

        # å€‹åˆ¥ã®æ¤ç‰©ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        if href and text and len(text) > 0:
          # å®Œå…¨ãªURLã«ã™ã‚‹
          if href.startswith('/'):
            full_url = "https://lovegreen.net" + href
          elif not href.startswith('http'):
            full_url = f"https://lovegreen.net/library/{category['type']}/" + href
          else:
            full_url = href

          # é™¤å¤–ã™ã‚‹URLãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿®æ­£ï¼ˆã‚ˆã‚Šå…·ä½“çš„ã«ï¼‰
          exclude_patterns = [
              f'/type/{category["type"]}/',      # ä¸€è¦§ãƒšãƒ¼ã‚¸è‡ªä½“ï¼ˆã‚ˆã‚Šå…·ä½“çš„ã«ï¼‰
              f'/type/{category["type"]}?',      # ä¸€è¦§ãƒšãƒ¼ã‚¸ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ã
              f'/type/{category["type"]}#',      # ä¸€è¦§ãƒšãƒ¼ã‚¸ã®ã‚¢ãƒ³ã‚«ãƒ¼ä»˜ã
              '/page/',                          # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
              '/category/',                      # ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸
              'syllabary=',                      # äº”åéŸ³é †æ¤œç´¢ãƒšãƒ¼ã‚¸
              f'?s&type={category["type"]}',     # æ¤œç´¢ãƒšãƒ¼ã‚¸
              '/search/',                        # æ¤œç´¢ãƒšãƒ¼ã‚¸
              '/tag/',                           # ã‚¿ã‚°ãƒšãƒ¼ã‚¸
              '/author/',                        # ä½œè€…ãƒšãƒ¼ã‚¸
              '/registration/',                  # ç™»éŒ²ãƒšãƒ¼ã‚¸
          ]

          # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
          should_exclude = False
          for pattern in exclude_patterns:
            if pattern in full_url:
              should_exclude = True
              break

          # äº”åéŸ³ï¼ˆ1æ–‡å­—ï¼‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚é™¤å¤–
          if len(text) == 1 and text in 'ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“':
            should_exclude = True

          # å€‹åˆ¥æ¤ç‰©ãƒšãƒ¼ã‚¸ã®åˆ¤å®šã‚’è¿½åŠ 
          # å€‹åˆ¥æ¤ç‰©ãƒšãƒ¼ã‚¸ã®URLãƒ‘ã‚¿ãƒ¼ãƒ³: /library/ã‚«ãƒ†ã‚´ãƒª/pæ•°å­—/ ã¾ãŸã¯ /library/ã‚«ãƒ†ã‚´ãƒª/æ¤ç‰©å/
          is_individual_page = False

          # pXXXXXå½¢å¼ã®ãƒšãƒ¼ã‚¸ID
          if '/p' in href and href.split('/p')[-1].rstrip('/').isdigit():
            is_individual_page = True

          # æ¤ç‰©åå½¢å¼ã®URLï¼ˆ/library/ã‚«ãƒ†ã‚´ãƒª/æ¤ç‰©å/ï¼‰
          url_parts = href.strip('/').split('/')
          if (len(url_parts) >= 3 and
              url_parts[0] == 'library' and
              url_parts[1] == category['type'] and
              len(url_parts[2]) > 1 and  # æ¤ç‰©åã¯1æ–‡å­—ä»¥ä¸Š
                  not url_parts[2].startswith('p')):  # pXXXXå½¢å¼ã§ãªã„
            is_individual_page = True

          # å€‹åˆ¥æ¤ç‰©ãƒšãƒ¼ã‚¸ã§é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã—ãªã„ã‚‚ã®ã®ã¿ã‚’è¿½åŠ 
          if is_individual_page and not should_exclude:
            page_plant_data.append(
                {"name": text, "url": full_url, "category": category['name']})
            print(f"âœ“ {text} - {full_url}")

      # å…ƒã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚‚è©¦ã™
      if len(cards) > 0:
        print(f"\n=== å…ƒã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§ã®æŠ½å‡º ===")
        for card in cards:
          link = card.get("href")
          name_tag = card.find("span", class_="library-list__item__title")
          name = name_tag.get_text(strip=True) if name_tag else None

          if name and link:
            full_url = link if link.startswith(
                "http") else "https://lovegreen.net" + link

            # åŒã˜é™¤å¤–ãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨
            exclude_patterns = [
                f'/type/{category["type"]}/',
                f'/type/{category["type"]}?',
                f'/type/{category["type"]}#',
                '/page/',
                '/category/',
                'syllabary=',
                f'?s&type={category["type"]}',
                '/search/',
                '/tag/',
                '/author/',
                '/registration/',
            ]

            should_exclude = False
            for pattern in exclude_patterns:
              if pattern in full_url:
                should_exclude = True
                break

            # äº”åéŸ³ï¼ˆ1æ–‡å­—ï¼‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚‚é™¤å¤–
            if len(name) == 1 and name in 'ã‚ã„ã†ãˆãŠã‹ããã‘ã“ã•ã—ã™ã›ããŸã¡ã¤ã¦ã¨ãªã«ã¬ã­ã®ã¯ã²ãµã¸ã»ã¾ã¿ã‚€ã‚ã‚‚ã‚„ã‚†ã‚ˆã‚‰ã‚Šã‚‹ã‚Œã‚ã‚ã‚’ã‚“':
              should_exclude = True

            # å€‹åˆ¥æ¤ç‰©ãƒšãƒ¼ã‚¸ã®åˆ¤å®š
            is_individual_page = False

            if '/p' in link and link.split('/p')[-1].rstrip('/').isdigit():
              is_individual_page = True

            url_parts = link.strip('/').split('/')
            if (len(url_parts) >= 3 and
                url_parts[0] == 'library' and
                url_parts[1] == category['type'] and
                len(url_parts[2]) > 1 and
                    not url_parts[2].startswith('p')):
              is_individual_page = True

            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ & é™¤å¤–ãƒã‚§ãƒƒã‚¯
            if (is_individual_page and not should_exclude and
                    not any(plant['url'] == full_url for plant in page_plant_data)):
              page_plant_data.append(
                  {"name": name, "url": full_url, "category": category['name']})
              print(f"âœ“ {name} - {full_url}")
            

      # ã“ã®ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¨ä½“ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆé‡è¤‡é™¤å»ï¼‰
      new_count = 0
      existing_urls = {plant['url'] for plant in all_plant_data}

      for plant in page_plant_data:
        if plant['url'] not in existing_urls:
          all_plant_data.append(plant)
          new_count += 1

      print(
          f"\n{category['name']} - ãƒšãƒ¼ã‚¸ {page_num}: {new_count}ä»¶ã®æ–°ã—ã„æ¤ç‰©ã‚’è¿½åŠ  (ç´¯è¨ˆ: {len(all_plant_data)}ä»¶)")

    except Exception as e:
      print(f"âŒ {category['name']} - ãƒšãƒ¼ã‚¸ {page_num} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
      continue

# ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ‚äº†
driver.quit()

print(f"\n{'='*80}")
print("å…¨ã‚«ãƒ†ã‚´ãƒªã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
print(f"{'='*80}")

# é‡è¤‡ã‚’é™¤å»ï¼ˆæœ€çµ‚ãƒã‚§ãƒƒã‚¯ï¼‰
unique_plants = []
seen_urls = set()
for plant in all_plant_data:
  if plant['url'] not in seen_urls:
    unique_plants.append(plant)
    seen_urls.add(plant['url'])

plant_data = unique_plants

# ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
print("\nã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ:")
category_stats = {}
for plant in plant_data:
  category = plant['category']
  category_stats[category] = category_stats.get(category, 0) + 1

for category, count in category_stats.items():
  print(f"  {category}: {count}ä»¶")

# --- çµ±åˆCSVã«ä¿å­˜ ---
if plant_data:
  with open("all_plants_urls.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=["name", "url", "category"])
    writer.writeheader()
    writer.writerows(plant_data)

  print(f"\nâœ… å®Œäº†ï¼š{len(plant_data)} ä»¶ã®æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’ all_plants_urls.csv ã«ä¿å­˜ã—ã¾ã—ãŸ")

  # å„ã‚«ãƒ†ã‚´ãƒªã®æœ€åˆã®3ä»¶ã‚’è¡¨ç¤º
  print("\nå„ã‚«ãƒ†ã‚´ãƒªã®æœ€åˆã®3ä»¶:")
  for category_name in ["é‡èœ", "æœæ¨¹", "èŠ±"]:
    category_plants = [p for p in plant_data if p['category'] == category_name]
    if category_plants:
      print(f"\n  ã€{category_name}ã€‘")
      for i, plant in enumerate(category_plants[:3]):
        print(f"    {i+1}. {plant['name']} - {plant['url']}")
      if len(category_plants) > 3:
        print(f"    ... ä»– {len(category_plants) - 3} ä»¶")

  # æœ€å¾Œã®5ä»¶ã‚’è¡¨ç¤º
  print(f"\n æœ€å¾Œã®5ä»¶:")
  for i, plant in enumerate(plant_data[-5:]):
    print(
        f"  {len(plant_data)-4+i}. [{plant['category']}] {plant['name']} - {plant['url']}")

else:
  print("\nâŒ æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

print(f"\nå‡¦ç†å®Œäº†ï¼åˆè¨ˆ {len(plant_data)} ä»¶ã®æ¤ç‰©ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ")
